%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
%%%% Small single column format, used for CIE, CSUR, DTRAP, JACM, JDIQ, JEA, JERIC, JETC, PACMCGIT, TAAS, TACCESS, TACO, TALG, TALLIP (formerly TALIP), TCPS, TDSCI, TEAC, TECS, TELO, THRI, TIIS, TIOT, TISSEC, TIST, TKDD, TMIS, TOCE, TOCHI, TOCL, TOCS, TOCT, TODAES, TODS, TOIS, TOIT, TOMACS, TOMM (formerly TOMCCAP), TOMPECS, TOMS, TOPC, TOPLAS, TOPS, TOS, TOSEM, TOSN, TQC, TRETS, TSAS, TSC, TSLP, TWEB.
% \documentclass[acmsmall]{acmart}

%%%% Large single column format, used for IMWUT, JOCCH, PACMPL, POMACS, TAP, PACMHCI
% \documentclass[acmlarge,screen]{acmart}

%%%% Large double column format, used for TOG
% \documentclass[acmtog, authorversion]{acmart}

%%%% Generic manuscript mode, required for submission
%%%% and peer review
\documentclass[manuscript,screen,review]{acmart}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{todonotes}
\usepackage{hyperref}

%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
%
%  Uncomment \acmBooktitle if th title of the proceedings is different
%  from ``Proceedings of ...''!
%
\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
 June 03--05, 2018, Woodstock, NY} 
\acmISBN{978-1-4503-XXXX-X/18/06}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title[Analyse von neuronal networks und climate change temperature data]{Analyse von neuronal network Architekturen im Bezug auf climate change temperature data}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Torge Schwark}
\authornote{Both authors contributed equally to this research.}
\email{stuTODO:@mail.uni-kiel.de}
\orcid{1234-5678-9012}
\author{Joschua Quotschalla}
\authornotemark[1]
\email{stu235352@mail.uni-kiel.de}
\affiliation{%
  \institution{Institute of Computer Science, University of Kiel}
  \streetaddress{P.O. Box 1212}
  \city{Kiel}
  \state{Schleswig-Holstein}
  \country{Germany}
  \postcode{2411TODO:}
}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
\todo[size=\small]{Abstract noch kürzer?}
Dieses Paper untersucht verschiedene neuronale Netzwerkarchitekturen 
im Kontext der Analyse von Klimawandel-Daten, insbesondere der Erdoberflächentemperatur. 
Die Architekturen umfassen MLPs, 1D Convolutional Neural Networks, LSTMs und Transformer. 
Die Analyse konzentriert sich auf den \textit{Climate Change: Earth Surface Temperature Data-Datensatz}. 
Dazu wird zunächst eine effiziente Data-Loader-Pipeline implementiert, Visualisierungen zum jeweiligen Datensatz 
als auch der Performance der Modelle erstellt und die Leistung der Architekturen qualitativ und quantitativ bewertet. 
Dabei weisen MLPs als auch 1D ConvNets ein sehr zuverlässiges und schnelles Training auf 
und bieten gleichzeitig die besten Ergebnisse. 
Die Studie beleuchtet auch Normalisierungseffekte, die Auswirkungen variabler Input-Sequenzen auf die Performance
und liefert Temperaturvorhersagen \textcolor{gray}{auf bis zu 100 Jahre in die Zukunft}, 
um weitere Schlüsse im Bezug auf den klimawandel ziehen zu können.
\end{abstract}

% %% TODO: Text in grau markiert unsichere/zu überarbeitende Stellen
% \begin{abstract}
% In Anbetracht der globalen Bedeutung und Auswirkungen des Klimawandels sind präzise Vorhersagen der zukünftigen Klimaentwicklung von entscheidender Bedeutung. 
% Die vorliegende Studie konzentriert sich auf die Anwendung von neuronalen Netzwerken zur Klimavorhersage, 
% wobei der umfangreiche Datensatz "Climate Change: Earth Surface Temperature Data" als Grundlage dient. 
% Durch die systematische Verwendung verschiedener Architekturen, darunter Multilayer-Perzeptronen (MLPs), Long Short-Term Memory Networks (LSTMs), 
% Convolutional Neural Networks (ConvNets) und Transformer-Modelle, wird das Ziel verfolgt, 
% präzise und langfristige Vorhersagen zur \textcolor{gray}{globalen Durchschnittstemperatur für die kommenden 100 Jahre zu ermöglichen.}
% \textcolor{gray}{Mit Blick auf die Dringlichkeit des Themas Klimawandel soll dieses Paper einen differenzierten Einblick in die Anwendung neuronaler Netze für die Klimavorhersage bieten. 
% Der Fokus liegt dabei nicht nur auf der Vorhersage zukünftiger Temperaturentwicklungen, sondern auch auf der Identifizierung langfristiger globaler Trends.} 
% Die Bedeutung und weitreichenden Folgen des Klimawandels für Mensch und Umwelt unterstreichen die Notwendigkeit, präzise und zuverlässige Vorhersagemodelle zu entwickeln, 
% um fundierte Entscheidungen und Maßnahmen treffen zu können.
% Neben der Motivation zur Untersuchung des Klimawandels aus datengetriebener Sicht werden in diesem Paper auch Einblicke in die angewandten Methoden, 
% den verwendeten Datensatz, den Trainingsprozess, die erzielten Ergebnisse und abschließende Schlussfolgerungen präsentiert. Das übergeordnete Ziel besteht darin, 
% einen bedeutenden Beitrag zur Vorhersage zukünftiger Klimaentwicklungen zu leisten und damit einen wertvollen Beitrag zum besseren Verständnis und zur Bewältigung des Klimawandels zu leisten.
% \textcolor{gray}{Diese Studie erstreckt sich über insgesamt 6 getippte Seiten, wobei dieser einleitende Abschnitt den notwendigen Rahmen für das folgende Forschungspapier bildet.}
% \end{abstract}

\keywords{neuronale Netzwerke, NN, MLP, 1D CONV, LSTM, Transformer, Klimawandel, Temperatur}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \begin{teaserfigure}
%   \includegraphics[scale=0.1]{./sources/NN_climate_icon.jpg}
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

% \section{Einleitung}
\section{Introduction}
Die vorliegende Arbeit widmet sich der ganzheitlichen Entwicklung 
und Evaluierung von neuronalen Netzwerkarchitekturen für die Klimavorhersage unter Verwendung eines ausgewählten Datensatzes. 
Angesichts der wachsenden Bedeutung präziser und zuverlässiger Klimavorhersagen für zahlreiche Anwendungsgebiete, 
vom landwirtschaftlichen Management bis hin zu infrastrukturellen Planungsprozessen, ist es von entscheidender Bedeutung, 
leistungsstarke und effektive Modelle zu entwickeln.
Das Ziel des vorliegenden Papers besteht darin, eine umfassende Analyse verschiedener Aspekte des Modelltrainings 
und der Architekturleistung durchzuführen, um fundierte Einblicke in die Vorhersagegenauigkeit 
und das Verhalten der Netzwerke zu gewinnen. 
Dabei liegt ein zentraler Schwerpunkt auf der Implementierung einer effizienten Data-Loader Pipeline, 
die darauf abzielt, Sequenzen von Datenpunkten effizient als Mini-Batches bereitzustellen. 
Durch die konsequente Anwendung bewährter Methoden \todo[size=\small]{Methoden nennen?} zur strukturierten Vorverarbeitung der Daten 
sollen optimale Voraussetzungen für das Training der neuronalen Netzwerke geschaffen werden. 
Darüber hinaus werden Visualisierungen für die Analyse der Daten angestrebt, 
um detaillierte Einblicke in die Strukturen und Charakteristika des Datensatzes als auch der Modelle zu gewinnen.
Im Zusammenhang dazu liegt ein weiterer zentraler Fokus dieser Arbeit 
auf der eingehenden Untersuchung der Leistung verschiedener Architekturen für neuronale Netzwerke, 
um die Frage zu beantworten, welche Architektur qualitativ und quantitativ die besten Vorhersageergebnisse erzielt. 
Dabei werden verschiedene Architekturen wie 1-dimensionale Convolutional Neural Networks (1D ConvNets), 
Multilayer-Perzeptronen (MLPs), Long Short-Term Memory Networks (LSTMs) und Transformer-Architekturen systematisch analysiert und verglichen.
Darüber hinaus wird die Wirkung der Datenormalisierung auf die Vorhersagegenauigkeit untersucht, 
indem die Performance der Modelle bei Verwendung von normalisierten und nicht-normalisierten Daten verglichen wird. 
Dies ermöglicht es, die Auswirkungen der Datenormalisierung auf die Modellleistung zu bewerten 
und präzise Rückschlüsse auf deren Bedeutung für die Vorhersagegenauigkeit zu ziehen.
Zusätzlich wird die Leistung verschiedener Architekturen in Abhängigkeit von der Länge der Input-Sequenzen 
von 8, 16, 32 und 64 analysiert \todo[size=\small]{Anpassen / erläutern, warum wir andere Werte haben (840, 300)}, 
um die Auswirkungen der Input-Sequenzlänge auf die Modellperformance zu untersuchen 
und wichtige Erkenntnisse für die spätere Modellauswahl zu gewinnen.
Insgesamt zielt diese Arbeit darauf ab, durch eine systematische und methodisch fundierte Vorgehensweise 
umfassende Einblicke in die Entwicklung und Leistungsfähigkeit von neuronalen Netzwerkarchitekturen für die Klimavorhersage zu gewinnen. Durch die Kombination von Vorverarbeitungstechniken, architekturspezifischen Analysen und Vergleichen sowie der Untersuchung verschiedener Einflussfaktoren auf die Modellperformance soll ein umfassendes Verständnis für die Gestaltung effektiver und präziser Klimavorhersagemodelle erlangt werden.

\section{Method}
\subsection*{Datensatz}
Datensatz: 




\todo[options]{}

\section{Datensatz}
\subsection*{Einführung und Datenverständnis}
Die Grundlage dieser Studie bildet der \href{https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data?select=GlobalLandTemperaturesByCity.csv}{"Climate Change: Earth Surface Temperature"} Datensatz. 
\textcolor{gray}{Ziel ist es, die Besonderheiten der Daten zu verstehen und eine strukturierte Organisierung für den Einsatz in neuronalen Netzwerken zu gewährleisten.} 
Dabei umfasst der genannte Datensatz unsgesamt 5 csv Dateien mit den folgenden Inhalten:
\todo[]{}
\begin{table}
  \caption{Komponenten Datensatzes}
  \label{tab:freq}
  \begin{tabular}{ccl}
    \toprule
    Beschriftung & Erläuterung\\
    \midrule
    GlobalLandTemperatureByCity & TODO \\
    GlobalLandTemperatureByCountry 1D & TODO \\
    GlobalLandTemperatureByMajorCity & TODO \\
    GlobalLandTemperatureByState & TODO \\
    GlobalLandTemperature & TODO \\
  \bottomrule
\end{tabular}
\end{table}
Die Temperaturen werden dabei von Float-Werten mit 16 Nachkommastellen auf zwei Nachkommastellen gerundet und nach geografischen Kategorien wie Land, Staat, Stadt und \textcolor{gray}{Hauptstadt} in separate Textdateien sortiert. 
Eine zunächst vermutete Optimierung bestand darin, die Daten beim Einlesen in ein Array zu speichern, um den Zugriff während des Trainings zu beschleunigen.
Nachdem dieser Ansatz aber keine messbaren Veränderungen mit sich brachte, wurden weitere Optimierungen bezüglich einer Parallelisierung auf der CPU und GPU untersucht, 
welche eine deutliche Beschleunigung des Trainingsprozesses von bis zu \textcolor{gray}{50\%} ermöglichten. 
Hierzu wird die Klasse Pool des Paketes multiprocessing verwendet, welche eine einfache Parallelisierung von Python-Code ermöglicht, indem eine Funktion mit mehreren Inputs auf mehrere Prozesse verteilt wird.


\subsection*{Data-Loader Pipeline}

Die Data-Loader Pipeline wurde implementiert, um Mini-Batches von Sequenzen von Datenpunkten gemäß den Anforderungen der Architekturen bereitzustellen. Dieser Prozess wurde optimiert, um die Effizienz des Trainings zu steigern. 
Zur Analyse wurden Visualisierungen wie Histogramme für geografische Merkmale, Datenverteilungen und Kartenplots verwendet. 
T-SNE-Plots ermöglichen eine multidimensionale Darstellung der Datenpunkte für eine bessere Analyse.

\section{Visualisierungen für Datenanalyse und Predictions}

\begin{figure}[htp]
  \centering
  \begin{subfigure}{.45\textwidth}
      \centering
      \includegraphics[width=.8\linewidth]{./histograms/map_plot_data_points}
      \caption{Histogram}
      \label{fig:sub1}
  \end{subfigure}%
  \begin{subfigure}{.45\textwidth}
      \centering
      \includegraphics[width=.8\linewidth]{./histograms/Uncertainty}
      \caption{Histogram 2}
      \label{fig:sub2}
  \end{subfigure}\\
  \begin{subfigure}{.45\textwidth}
      \centering
      \includegraphics[width=.8\linewidth]{./histograms/Temperatur}
      \caption{Histogram 3}
      \label{fig:sub3}
  \end{subfigure}%
  \begin{subfigure}{.45\textwidth}
      \centering
      \includegraphics[width=.8\linewidth]{./histograms/Continents}
      \caption{Histogram 4}
      \label{fig:sub4}
  \end{subfigure}
  \caption{Four histograms}
  \label{fig:test}
  \Description{Histograms caputuring the distribution of the data.}
\end{figure}

Die Implementierung verschiedener Visualisierungen spielt eine entscheidende Rolle bei der Analyse von Daten und Vorhersagen. 
Hierbei wurden Histogramme für den Längen- und Breitengrad, die Verteilung von Datenlücken, Temperaturverteilungen und Unsicherheiten erstellt. 
Zusätzlich wurde ein Kartenplot der Datenpunkte generiert. 
Diese Visualisierungen bieten Einblicke in die geografische Verteilung der Daten und ermöglichen eine detaillierte Analyse von Städten mit identischen Werten.


\section{Training-Procedure}

\begin{table}
  \caption{Network Architekturen}
  \label{tab:freq}
  \begin{tabular}{ccl}
    \toprule
    Netzwerk&Anzahl Parameter&Anzahl Layern\\
    \midrule
    MLP & TODO: & meisten Parameter aufgrund von dense layern\\
    CONV 1D & TODO: & geringere Anzahl durch feature extraction convolution layer vor den dense layern\\
    LSTM & TODO: & TODO:\\
    Transformer & TODO: & (zweit) meisten Parameter aufgrund von mehreren inneren MLPs\\
  \bottomrule
\end{tabular}
\end{table}

Die Netzwerkarchitekturen MLP, 1D Conv Net, LSTM und Transformer wurden auf den vorbereiteten Datensätzen trainiert. 
Im ersten Schritt wurde eine Grid Search durchgeführt, um die optimalen Modelle und deren Hyperparameter zu bestimmen. 
Nach dieser automatisierten Suchstrategie wurde ein händisches Fine-Tuning durchgeführt, um die Leistung der Modelle weiter zu verbessern. 
Während des gesamten Prozesses wurde darauf geachtet, wie sich jede Architektur an die spezifischen Charakteristiken des Datensatzes anpasst und 
wie gut sie die Temperaturvorhersagen durchführen kann. 
Die sorgfältige Auswahl und Anpassung der Hyperparameter und Aktivierungsfunktionen spielte eine entscheidende Rolle bei der Optimierung der Modellleistung.

Rewe GPT:
Optimierung von Architekturen und Parametern
Die Auswahl und Optimierung der geeigneten Parameter für neuronale Netzwerkarchitekturen spielt eine entscheidende Rolle bei der Leistungsfähigkeit und Zuverlässigkeit der Klimavorhersagemodelle. 
Im Rahmen der Untersuchung wurden verschiedene Schlüsselparameter betrachtet und optimiert, um die bestmöglichen Ergebnisse zu erzielen.

\textbf{MLP (Multilayer-Perzeptronen)}
Im Kontext von Multilayer-Perzeptronen (MLPs) spielen insbesondere die Anzahl der Hidden Layers, die Anzahl der Neuronen pro Layer, 
die Wahl der Aktivierungsfunktionen und die Lernrate eine bedeutsame Rolle bei der Optimierung. 
Die Anpassung dieser Parameter beeinflusst maßgeblich die Fähigkeit des MLP-Modells, komplexe nichtlineare Zusammenhänge zu erfassen und präzise Vorhersagen zu generieren. 
Unsere Experimente haben gezeigt, dass die Auswahl und Feinabstimmung dieser Parameter entscheidend sind, um eine ausgewogene Modellkapazität zu gewährleisten und \textcolor{gray}{Überanpassung/Overfitting} zu vermeiden.
Die systematische Optimierung der genannten Architekturen und Parameter bildet die Grundlage für die Evaluierung ihrer Leistungsfähigkeit im Kontext der Klimavorhersage. 
Es ist wichtig, die spezifischen Verhaltensweisen und Interaktionen der Optimierungsergebnisse zu analysieren, um fundierte Schlussfolgerungen hinsichtlich der Vorhersagequalität und der Quantifizierung des Modellverhaltens ziehen zu können.

\textbf{LSTMs (Long Short-Term Memory Networks)}
Ein zentraler Parameter bei LSTMs ist die Anzahl der Memory Units oder sogenannten Neuronen in der Zelle. Durch gezielte Anpassung dieser Anzahl kann die Netzwerkleistung verbessert 
und die Fähigkeit zur Erfassung von langfristigen Abhängigkeiten gesteuert werden. 
In dem Experimenten wurde festgestellt, dass eine moderate Anzahl von Memory Units zu einer ausgewogenen Leistung führt, während zu wenige Units die Kapazität des Netzwerks begrenzen und zu schlechterer Modellleistung führen können.

\textbf{Convolutional Neural Networks (ConvNets)}
Für ConvNets sind insbesondere die Größe und Anordnung der \textcolor{gray}{Kernel/Filter} sowie deren Anzahl in den Convolutional Layern von großer Bedeutung. 
Während größere \textcolor{gray}{Kernel/Filter} und eine höhere Anzahl von Filtern zu einer komplexeren Modellarchitektur und möglicherweise zu einer besseren Erfassung räumlicher Merkmale führen können, 
besteht die Herausforderung darin, ein ausgewogenes Verhältnis zu finden, das Overfitting vermeidet und effektivere Modelle ermöglicht.

\textbf{Transformer-Architekturen}
Bei der Optimierung von Transformer-Modellen, insbesondere in Bezug auf Klimavorhersagen, sind die Anzahl der Layer, die Dimensionalität der Embeddings, die Anzahl der Köpfe in den Multi-Head Attention Mechanismen und die Lernrate wichtige Parameter. Unsere Untersuchungen haben gezeigt, dass die sorgfältige Abstimmung dieser Parameter in einem komplexen Zusammenspiel entscheidend ist, um eine optimale Leistungsfähigkeit der Transformer-Architekturen zu erreichen.
Es ist wichtig zu betonen, dass Optimierungsansätze stark von den spezifischen Merkmalen des betrachteten Datensatzes und den Zielsetzungen der Klimavorhersagen abhängen. Daher ist eine systematische und gewissenhafte Evaluierung der Optimierungsergebnisse unerlässlich, um fundierte Schlussfolgerungen hinsichtlich der Performanz der unterschiedlichen Architekturen zu ziehen.


\subsection{Architekturvergleich und Normalisierungen}

Die Leistung der Architekturen wurde sowohl qualitativ als auch quantitativ bewertet. Aktivierungsfunktionen wie Selu und Relu wurden für MLP analysiert. Bei 1D Conv Net wurde die Auswirkung von Global Average Pooling und Flatten vor den Dense Layern untersucht. LSTM und Transformer wurden auf ihre spezifischen Eigenschaften und Fehlerquellen analysiert. Der Vergleich erfolgte durch Bewertung der qualitativen Anpassung und quantitativen Metriken wie dem Mean Absolute Error (MAE).


\subsection{Normalisierung und Variation der Input-Sequenzen}

Die Daten wurden vor dem Training normalisiert, und die Auswirkungen auf die Vorhersagegenauigkeit wurden verglichen. Zusätzlich wurde die Performance der Modelle für variable Input-Sequenzen von 8, 16, 32 und 64 untersucht. Dies ermöglichte eine tiefgreifende Analyse, wie die Länge der Input-Sequenzen die Modellleistung beeinflusst.


Insgesamt bietet diese Studie eine umfassende Analyse der Climate Change-Daten unter Verwendung verschiedener Netzwerkarchitekturen. Die gewonnenen Erkenntnisse ermöglichen nicht nur eine bessere Anpassung der Modelle an die Daten, sondern auch Optimierungsmöglichkeiten für zukünftige Forschungen im Bereich des Klimawandels.

\section{Results}
\todo[size=\small]{TODO}

\section{Conclusion}
\todo[size=\small]{TODO}

\section{Citations and Bibliographies}

The use of \BibTeX\ for the preparation and formatting of one's
references is strongly recommended. Authors' names should be complete
--- use full first names (``Donald E. Knuth'') not initials
(``D. E. Knuth'') --- and the salient identifying features of a
reference should be included: title, year, volume, number, pages,
article DOI, etc.

The bibliography is included in your source document with these two
commands, placed just before the \verb|\end{document}| command:
\begin{verbatim}
  \bibliographystyle{ACM-Reference-Format}
  \bibliography{bibfile}
\end{verbatim}
where ``\verb|bibfile|'' is the name, without the ``\verb|.bib|''
suffix, of the \BibTeX\ file.

Citations and references are numbered by default. A small number of
ACM publications have citations and references formatted in the
``author year'' style; for these exceptions, please include this
command in the {\bfseries preamble} (before the command
``\verb|\begin{document}|'') of your \LaTeX\ source:
\begin{verbatim}
  \citestyle{acmauthoryear}
\end{verbatim}

  Some examples.  A paginated journal article \cite{Abril07}, an
  enumerated journal article \cite{Cohen07}, a reference to an entire
  issue \cite{JCohen96}, a monograph (whole book) \cite{Kosiur01}, a
  monograph/whole book in a series (see 2a in spec. document)
  \cite{Harel79}, a divisible-book such as an anthology or compilation
  \cite{Editor00} followed by the same example, however we only output
  the series if the volume number is given \cite{Editor00a} (so
  Editor00a's series should NOT be present since it has no vol. no.),
  a chapter in a divisible book \cite{Spector90}, a chapter in a
  divisible book in a series \cite{Douglass98}, a multi-volume work as
  book \cite{Knuth97}, a couple of articles in a proceedings (of a
  conference, symposium, workshop for example) (paginated proceedings
  article) \cite{Andler79, Hagerup1993}, a proceedings article with
  all possible elements \cite{Smith10}, an example of an enumerated
  proceedings article \cite{VanGundy07}, an informally published work
  \cite{Harel78}, a couple of preprints \cite{Bornmann2019,
    AnzarootPBM14}, a doctoral dissertation \cite{Clarkson85}, a
  master's thesis: \cite{anisi03}, an online document / world wide web
  resource \cite{Thornburg01, Ablamowicz07, Poker06}, a video game
  (Case 1) \cite{Obama08} and (Case 2) \cite{Novak03} and \cite{Lee05}
  and (Case 3) a patent \cite{JoeScientist001}, work accepted for
  publication \cite{rous08}, 'YYYYb'-test for prolific author
  \cite{SaeediMEJ10} and \cite{SaeediJETC10}. Other cites might
  contain 'duplicate' DOI and URLs (some SIAM articles)
  \cite{Kirschmer:2010:AEI:1958016.1958018}. Boris / Barbara Beeton:
  multi-volume works as books \cite{MR781536} and \cite{MR781537}. A
  couple of citations with DOIs:
  \cite{2004:ITE:1009386.1010128,Kirschmer:2010:AEI:1958016.1958018}. Online
  citations: \cite{TUGInstmem, Thornburg01, CTANacmart}. Artifacts:
  \cite{R} and \cite{UMassCitations}.

\section{Acknowledgments}

Identification of funding sources and other support, and thanks to
individuals and groups that assisted in the research and the
preparation of the work should be included in an acknowledgment
section, which is placed just before the reference section in your
document.

This section has a special environment:
\begin{verbatim}
  \begin{acks}
  ...
  \end{acks}
\end{verbatim}
so that the information contained therein can be more easily collected
during the article metadata extraction phase, and to ensure
consistency in the spelling of the section heading.

Authors should not prepare this section as a numbered or unnumbered {\verb|\section|}; please use the ``{\verb|acks|}'' environment.

\section{Appendices}

If your work needs an appendix, add it before the
``\verb|\end{document}|'' command at the conclusion of your source
document.

Start the appendix with the ``\verb|appendix|'' command:
\begin{verbatim}
  \appendix
\end{verbatim}
and note that in the appendix, sections are lettered, not
numbered. This document has two appendices, demonstrating the section
and subsection identification method.

\section{Multi-language papers}

Papers may be written in languages other than English or include
titles, subtitles, keywords and abstracts in different languages (as a
rule, a paper in a language other than English should include an
English title and an English abstract).  Use \verb|language=...| for
every language used in the paper.  The last language indicated is the
main language of the paper.  For example, a French paper with
additional titles and abstracts in English and German may start with
the following command
\begin{verbatim}
\documentclass[sigconf, language=english, language=german,
               language=french]{acmart}
\end{verbatim}

The title, subtitle, keywords and abstract will be typeset in the main
language of the paper.  The commands \verb|\translatedXXX|, \verb|XXX|
begin title, subtitle and keywords, can be used to set these elements
in the other languages.  The environment \verb|translatedabstract| is
used to set the translation of the abstract.  These commands and
environment have a mandatory first argument: the language of the
second argument.  See \verb|sample-sigconf-i13n.tex| file for examples
of their usage.

\section{SIGCHI Extended Abstracts}

The ``\verb|sigchi-a|'' template style (available only in \LaTeX\ and
not in Word) produces a landscape-orientation formatted article, with
a wide left margin. Three environments are available for use with the
``\verb|sigchi-a|'' template style, and produce formatted output in
the margin:
\begin{itemize}
\item {\verb|sidebar|}:  Place formatted text in the margin.
\item {\verb|marginfigure|}: Place a figure in the margin.
\item {\verb|margintable|}: Place a table in the margin.
\end{itemize}

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
To Robert, for the bagels and explaining CMYK and color spaces.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

%%
%% If your work has an appendix, this is the place to put it.
\appendix

\section{Research Methods}

\subsection{Part One}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi
malesuada, quam in pulvinar varius, metus nunc fermentum urna, id
sollicitudin purus odio sit amet enim. Aliquam ullamcorper eu ipsum
vel mollis. Curabitur quis dictum nisl. Phasellus vel semper risus, et
lacinia dolor. Integer ultricies commodo sem nec semper.

\subsection{Part Two}

Etiam commodo feugiat nisl pulvinar pellentesque. Etiam auctor sodales
ligula, non varius nibh pulvinar semper. Suspendisse nec lectus non
ipsum convallis congue hendrerit vitae sapien. Donec at laoreet
eros. Vivamus non purus placerat, scelerisque diam eu, cursus
ante. Etiam aliquam tortor auctor efficitur mattis.

\section{Online Resources}

Nam id fermentum dui. Suspendisse sagittis tortor a nulla mollis, in
pulvinar ex pretium. Sed interdum orci quis metus euismod, et sagittis
enim maximus. Vestibulum gravida massa ut felis suscipit
congue. Quisque mattis elit a risus ultrices commodo venenatis eget
dui. Etiam sagittis eleifend elementum.

Nam interdum magna at lectus dignissim, ac dignissim lorem
rhoncus. Maecenas eu arcu ac neque placerat aliquam. Nunc pulvinar
massa et mattis lacinia.

\end{document}
\endinput
%%
%% End of file `sample-authordraft.tex'.